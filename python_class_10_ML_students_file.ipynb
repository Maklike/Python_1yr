{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center\"> Introduction to Python and Machine Learning\n",
    "\n",
    "## <div style=\"text-align: center\">Machine Learning in Python (III) - Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](data:image/jpeg;base64,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)\n",
    "![logo2](https://efs.mrpips.gov.pl/__data/assets/image/0014/11336/04_zestawienie_power_rp_ue_efs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to display the points we use the predefined (by us) `plot_decision` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following means \"from imports_for_ML.py import everything\"\n",
    "from imports_for_ML import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "üì∫ ************* WATCH AT HOME - a WONDERFUL introduction to Artificial Neural Networks *************\n",
    "<br>\n",
    "\n",
    "https://www.3blue1brown.com/neural-networks\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "Our plan:\n",
    "* Introduction to the Keras library.\n",
    "* The simplest neural network, equivalent of logistic regression\n",
    "* Examples of neural network behavior with different number of neurons and layers for simple data with two predictive factors\n",
    "* A neural network implemented on Titanic data\n",
    "\n",
    "Then we will move on to the use of neural networks in image analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "üì∫ ************* WATCH AT HOME *************\n",
    "<br>\n",
    "\n",
    "[Keras Explained](https://www.youtube.com/watch?v=j_pJmXJwMLA)\n",
    "\n",
    "[Layers in a Neural Network explained](https://www.youtube.com/watch?v=FK77zZxaBoI)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "üì∫ ************* WATCH AT HOME *************\n",
    "<br>\n",
    "\n",
    "[Which Activation Function Should I Use? - VERY GOOD VIDEO about RELU!](https://www.youtube.com/watch?v=-7scQpJT7uo)\n",
    "\n",
    "[Activation Functions in a Neural Network explained](https://www.youtube.com/watch?v=m0pIlLfpXWE)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras` is the simplest library for neural networks in Python. It is high-level, which means that many details are done automatically, and allows us to focus on the \"big picture\". Keras is known for its well-designed API (programming interface), which makes it very convenient to use. Today we will use a simple sequential network (`Sequential`), for which subsequent layers are connected on a case-by-case basis (`Dense` type connection).  \n",
    "\n",
    "Installing keras (from the `Anaconda Prompt` application): \n",
    "\n",
    "`conda install keras`\n",
    "\n",
    "OR\n",
    "\n",
    "`pip install keras`\n",
    "\n",
    "---\n",
    "\n",
    "We will import the appropriate objects first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "‚ö†Ô∏èTASK 1\n",
    "<br>\n",
    "\n",
    "We will also create an artificial dataset using the `make_moons` function. We would like to create `250` points, with a noise level of `0.25` and a `random_state` as `23` (to get similar results on the n-th run of our function):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a348373b2bb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_moons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_moon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_moon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_moons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m___\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m___\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_moon, y_moon = make_moons(n_samples=___, noise=___, random_state=__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `plot_decision` function we already know to display the data we have created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "‚ö†Ô∏èTASK 2\n",
    "<br>\n",
    "\n",
    "But this time we will check the correctness on the training and test data. Therefore, we divide the data into test and training data. Let the test data account for 25% of all data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moon_train, X_moon_test, y_moon_train, y_moon_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a sequential network, we first use `Sequential` - this way we will create an empty space (without layers yet) - to which we will \"stick\" the consequtive layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "‚ö†Ô∏èTASK 3\n",
    "<br>\n",
    "\n",
    "Apply a \"dense\" layer first. (`Dense`) - i.e. one in which each neuron is connected to each other in the previous layer.  \n",
    "* The first argument for the `Dense` is the number of neurons - we want only one neuron for now.\n",
    "* We also want to set the type of activation for the added layer; the activation determines how the neurons transform the incoming signal. Because this one added neuron will also be an output neuron, we want it to have an activation of the `'sigmoid'` type (like a logistic function) - i.e. a minimum value of 0 and a maximum of 1. The activity of this neuron will determine whether the point belongs to the blue group (value zero) or to the orange group (value one).\n",
    "* The neuron added by us is simultaneously the output neuron and the first one adjacent to the data, therefore it needs to know what is the size of this data (`input_shape`). Our data has two predictors (values on the x-axis and values on the y-axis), so we give `input_shape=(2,)` (we give dimensions as a *tuple* type `(2,)`. Because the data could have e.g. height and width as in the case of photos, we would give `(height, width)`). \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(_, activation=_________, input_shape=(_,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display a summary of the created model, use the `summary` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first network is very modest - only 3 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compile the model, i.e. prepare it to run. When compiling, we specify the cost function (`loss`), the optimization method (`optimizer`). We can also ask for some values to be calculated for the next steps of fitting the model (`metrics`).  \n",
    "* We want the cost function to be `'binary_crossentropy'` (we will always use it for classification problems).\n",
    "* Optimizer is set to `'adam'`, a popular optimization method that often leads to faster network learning.\n",
    "* We also want to calculate the correctness for the next steps of network fit (`['accuracy']`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=_________________, optimizer=______, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to fit the model to the data, we use the familiar method `.fit` - we provide the training data to it: predictors and classifications. We also want to define how long the network is going to learn, most often we define it in the so-called epochs. One epoch is the use of all observations in training. Let us choose `250` epochs. Additionally, we use the `verbose` argument and set it to `0` in order not to clutter the screen with messages about training progress (for such small data the training will be very fast, we do not need information about progress)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.___(__________, _________, epochs=___, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the fitting, we can retrieve the correctness information for the next matching steps (subsequent epochs) from the `history` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your correctness is below 80%, you can run the model fitting cell (`model.fit(...)`) again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that the solution reached by the network is tantamount to the logistic regression that we have already learned.\n",
    "This is because our network weighs predictors (values on the x and y axes in the graphic representation), adds up and transforms them logistically. Logistic regression does exactly the same - it selects weights for the predictors, adds up and transforms logistically. Logistic regression still has an `intercept`, but it is no different in the case of neural networks - every neuron has a so-called `bias`, which works exactly the same way as the `intercept`. As a result, we have implemented logistic regression using a neural network. We need more layers to be able to achieve better results.  \n",
    "However, before we move on to adding layers, we will check what correctness we get on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_correctness(X, y, clf, threshold=0.5):\n",
    "    probabilities = clf.predict(X)[:, 0]\n",
    "    predictions = (probabilities > threshold).astype('int')\n",
    "    correctness = (predictions == y).mean()\n",
    "    print('The correctness is: {:.2f}'.format(correctness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_correctness(__________, __________, ______)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to increase model correctness. We will create a network with one hidden layer. We create the model the same way as before, we also create the output layer. Now, before we add the output layer, we add another layer with the following characteristics:\n",
    "* 3 neurons\n",
    "* Activation of the `relu` type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "üì∫ ************* WATCH AT HOME *************\n",
    "<br>\n",
    "\n",
    "[Which Activation Function Should I Use? - VERY GOOD VIDEO about RELU!](https://www.youtube.com/watch?v=-7scQpJT7uo)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(_, activation=________, input_shape=(2,)))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile and fit the model as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=_________________, optimizer=_______, metrics=___________)\n",
    "history = model.fit(_____________, ____________, epochs=250, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "‚ö†Ô∏èTASK 4\n",
    "<br>\n",
    "\n",
    "Now check the correctness of this network for the test data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "‚ö†Ô∏èTASK 5\n",
    "<br>\n",
    "\n",
    "Create a model with two hidden layers: one with 8 neurons, the other with 2. Both hidden layers with activation of `'relu'` type.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model\n",
    "model = Sequential()\n",
    "\n",
    "# hidden layers\n",
    "model.add(Dense(_, activation=______, input_shape=(2,)))\n",
    "model.add(Dense(_, activation=______))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling, training, and plotting the accuracy history\n",
    "model.compile(loss=_________________, optimizer=_______, metrics=___________)\n",
    "history = model.fit(_____________, ____________, epochs=350, verbose=0)\n",
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "‚ö†Ô∏èTASK 6\n",
    "<br>\n",
    "\n",
    "Now check the correctness of this network for the test data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "‚ö†Ô∏èTASK 7\n",
    "<br>\n",
    "\n",
    "Build the same model as in the first exercise, but now the second hidden layer should have 10 neurons.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the code creating the network\n",
    "model = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=______, metrics=['accuracy'])\n",
    "history = model.fit(X_moon_train, y_moon_train, epochs=350, verbose=0)\n",
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision(_________, __________, clf=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "‚ö†Ô∏èTASK 8\n",
    "<br>\n",
    "\n",
    "Now check the correctness of this network for the test data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see what the correctness is right now???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the Titanic\n",
    "First, create a network with two hidden layers - 10 neurons each.\n",
    "We will now use titanic data; also note that we have a different number of predicators here, which we have to provide to `input_shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_preproc = pd.read_csv('titanic_preproc.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sns.load_dataset('titanic').survived.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    titanic_preproc.values, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "X_train_std = std.fit_transform(X_train)\n",
    "X_test_std = std.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "model = Sequential()\n",
    "model.add(Dense(_, activation=______, input_shape=(__,)))\n",
    "model.add(Dense(_, activation=______))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During model training we would like to see how the correctness of the test data changes with training.\n",
    "We can do this by giving an additional argument of `validation_data` and setting it to `(X_test_std, y_test)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and showing accuracy\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(________, ________, epochs=400, verbose=0,\n",
    "                    validation_data=______________________________)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on the training data:')\n",
    "show_correctness(X_train_std, y_train, model)\n",
    "\n",
    "print('\\Accuracy on the test data:')\n",
    "show_correctness(X_test_std, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of previous algorithms, in the case of neural networks we are also dealing with hyperparameters. The most obvious hyperparameters of neural networks are: \n",
    "- the number of layers, \n",
    "- number of neurons in each layer,\n",
    "- activation types,\n",
    "- regularization (regular or dropout - but we will talk about it later), \n",
    "- finally: matching algorithm, learning rate (how big steps in the searched parameter space are performed) and others. \n",
    "\n",
    "We won't learn how to match hyperparameters for the network (apart from choosing the number of layers and neurons), but it's worth remembering that there is such a possibility. Professional applications of neural networks usually perform tests on many lists of hyperparameters (especially those hyperparameters that are not network architectures)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
